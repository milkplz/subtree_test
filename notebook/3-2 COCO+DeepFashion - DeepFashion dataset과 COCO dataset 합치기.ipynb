{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "DIR_ROOT = os.getcwd()\n",
    "DIR_DEEP = os.path.join(DIR_ROOT, \"../dataset/deepfashion\")\n",
    "DIR_COCO = os.path.join(DIR_ROOT, \"../dataset/coco\")\n",
    "DIR_ALL = os.path.join(DIR_ROOT, \"../dataset/all\")\n",
    "\n",
    "# Deepfashion dataset\n",
    "FILE_DEEPFASHION_TRAIN_JSON = os.path.join(DIR_ROOT, DIR_DEEP, \"annotations\", \"instances_train2017.json\")\n",
    "FILE_DEEPFASHION_VAL_JSON = os.path.join(DIR_ROOT, DIR_DEEP, \"annotations\", \"instances_val2017.json\")\n",
    "DIR_ROOT_DEEPFASHION_IMAGES = os.path.join(DIR_ROOT, DIR_DEEP, \"images\")\n",
    "\n",
    "# COCO dataset\n",
    "FILE_COCO_TRAIN_JSON = os.path.join(DIR_ROOT, DIR_COCO, \"annotations\", \"instances_train2017.json\")\n",
    "FILE_COCO_VAL_JSON = os.path.join(DIR_ROOT, DIR_COCO, \"annotations\", \"instances_val2017.json\")\n",
    "DIR_ROOT_COCO_IMAGES_TRAIN2017 = os.path.join(DIR_ROOT, DIR_COCO, \"images/train2017\")\n",
    "DIR_ROOT_COCO_IMAGES_VAL2017 = os.path.join(DIR_ROOT, DIR_COCO, \"images/val2017\")\n",
    "\n",
    "# new dataset\n",
    "DIR_ALL_ANNOTATIONS = os.path.join(DIR_ALL, \"annotations\")\n",
    "DIR_ALL_IMAGES_TRAIN2017 = os.path.join(DIR_ALL, \"images/train2017\")\n",
    "DIR_ALL_IMAGES_VAL2017 = os.path.join(DIR_ALL, \"images/val2017\")\n",
    "FILE_ALL_TRAIN_JSON = os.path.join(DIR_ALL_ANNOTATIONS, \"instances_train2017.json\")\n",
    "FILE_ALL_VAL_JSON = os.path.join(DIR_ALL_ANNOTATIONS, \"instances_val2017.json\")\n",
    "FILE_DEEP_TRAIN_JSON = os.path.join(DIR_ALL_ANNOTATIONS, \"instances_train2017.json\")\n",
    "FILE_DEEP_VAL_JSON = os.path.join(DIR_ALL_ANNOTATIONS, \"instances_val2017.json\")\n",
    "\n",
    "# FILE_COCO_TRAIN_JSON = os.path.join(DIR_ALL_ANNOTATIONS, \"coco_train2017.json\")\n",
    "# FILE_COCO_VAL_JSON = os.path.join(DIR_ALL_ANNOTATIONS, \"coco_val2017.json\")\n",
    "\n",
    "if not os.path.exists(DIR_ALL_ANNOTATIONS):\n",
    "    os.makedirs(DIR_ALL_ANNOTATIONS)\n",
    "    \n",
    "if not os.path.exists(DIR_ALL_IMAGES_TRAIN2017):\n",
    "    os.makedirs(DIR_ALL_IMAGES_TRAIN2017)\n",
    "\n",
    "if not os.path.exists(DIR_ALL_IMAGES_VAL2017):\n",
    "    os.makedirs(DIR_ALL_IMAGES_VAL2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepfashion dataset과 COCO 2017 dataset 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_train = json.load(open(FILE_COCO_TRAIN_JSON))\n",
    "coco_val = json.load(open(FILE_COCO_VAL_JSON))\n",
    "deep_train = json.load(open(FILE_DEEPFASHION_TRAIN_JSON))\n",
    "deep_val = json.load(open(FILE_DEEPFASHION_VAL_JSON))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load된 data 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'segmentation': [[239.97, 260.24, 222.04, 270.49, 199.84, 253.41, 213.5, 227.79, 259.62, 200.46, 274.13, 202.17, 277.55, 210.71, 249.37, 253.41, 237.41, 264.51, 242.54, 261.95, 228.87, 271.34]], 'area': 2765.1486500000005, 'iscrowd': 0, 'image_id': 558840, 'bbox': [199.84, 200.46, 77.71, 70.88], 'category_id': 58, 'id': 156}\n",
      "{'supercategory': 'person', 'id': 1, 'name': 'person'}\n",
      "{'license': 3, 'file_name': '000000391895.jpg', 'coco_url': 'http://images.cocodataset.org/train2017/000000391895.jpg', 'height': 360, 'width': 640, 'date_captured': '2013-11-14 11:18:45', 'flickr_url': 'http://farm9.staticflickr.com/8186/8119368305_4e622c8349_z.jpg', 'id': 391895}\n",
      "{'description': 'COCO 2017 Dataset', 'url': 'http://cocodataset.org', 'version': '1.0', 'year': 2017, 'contributor': 'COCO Consortium', 'date_created': '2017/09/01'}\n",
      "{'url': 'http://creativecommons.org/licenses/by-nc-sa/2.0/', 'id': 1, 'name': 'Attribution-NonCommercial-ShareAlike License'}\n",
      "{'area': 29854, 'bbox': [43, 1, 118, 253], 'category_id': 126, 'id': 1000000154443, 'image_id': 1000000154443, 'iscrowd': 0, 'segmentation': []}\n",
      "{'id': 102, 'name': 'Blazer', 'supercategory': 'upper-body'}\n",
      "260299\n",
      "28923\n"
     ]
    }
   ],
   "source": [
    "print(coco_train['annotations'][0])\n",
    "print(coco_train['categories'][0])\n",
    "print(coco_train['images'][0])\n",
    "print(coco_train['info'])\n",
    "print(coco_train['licenses'][0])\n",
    "\n",
    "print(deep_train['annotations'][1])\n",
    "print(deep_train['categories'][1])\n",
    "print(len(deep_train['images']))\n",
    "print(len(deep_val['images']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCO dataset과 Deepfashion dataset을 합치고 JSON파일로 저장\n",
    "\n",
    "1.두 dataset들을 하나의 dataset으로 만듭니다.\n",
    "2.images data는 merge한 후, shuffle 시킵니다. (coco data 다음에 deepfashion data가 위치하므로)\n",
    "3.json 파일로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "coco_train_anno = coco_train['annotations']\n",
    "deep_train_anno = deep_train['annotations']\n",
    "all_train_anno = coco_train_anno+deep_train_anno\n",
    "\n",
    "coco_val_anno = coco_val['annotations']\n",
    "deep_val_anno = deep_val['annotations']\n",
    "all_val_anno = coco_val_anno+deep_val_anno\n",
    "\n",
    "coco_train_img = coco_train['images']\n",
    "deep_train_img = deep_train['images']\n",
    "all_train_img = coco_train_img+deep_train_img\n",
    "\n",
    "# random.shuffle(all_train_img)\n",
    "\n",
    "coco_val_img = coco_val['images']\n",
    "deep_val_img = deep_val['images']\n",
    "all_val_img = coco_val_img+deep_val_img\n",
    "\n",
    "# random.shuffle(all_val_img)\n",
    "\n",
    "coco_cate = coco_train['categories']\n",
    "deep_cate = deep_train['categories']\n",
    "all_cate = coco_cate+deep_cate\n",
    "\n",
    "all_info = coco_train['info']\n",
    "all_licenses = coco_train['licenses']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('start - make json file')\n",
    "train_data = {}\n",
    "train_data['annotations'] = all_train_anno\n",
    "train_data['categories'] = all_cate\n",
    "train_data['images'] = all_train_img\n",
    "with open(FILE_ALL_TRAIN_JSON, 'w') as outfile:\n",
    "    json.dump(train_data, outfile)\n",
    "    \n",
    "    \n",
    "val_data = {}\n",
    "val_data['annotations'] = all_val_anno\n",
    "val_data['categories'] = all_cate\n",
    "val_data['images'] = all_val_img\n",
    "with open(FILE_ALL_VAL_JSON, 'w') as outfile:\n",
    "    json.dump(val_data, outfile)\n",
    "print('end - make json file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_train_data = {}\n",
    "coco_train_data['annotations'] = coco_train_anno\n",
    "coco_train_data['categories'] = all_cate\n",
    "coco_train_data['images'] = coco_train_img\n",
    "with open(FILE_COCO_TRAIN_JSON, 'w') as outfile:\n",
    "    json.dump(coco_train_data, outfile)\n",
    "    \n",
    "coco_val_data = {}\n",
    "coco_val_data['annotations'] = coco_val_anno\n",
    "coco_val_data['categories'] = all_cate\n",
    "coco_val_data['images'] = coco_val_img\n",
    "with open(FILE_COCO_VAL_JSON, 'w') as outfile:\n",
    "    json.dump(coco_val_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_train_data = {}\n",
    "deep_train_data['annotations'] = deep_train_anno\n",
    "deep_train_data['categories'] = all_cate\n",
    "deep_train_data['images'] = deep_train_img\n",
    "with open(FILE_DEEP_TRAIN_JSON, 'w') as outfile:\n",
    "    json.dump(deep_train_data, outfile)\n",
    "    \n",
    "deep_val_data = {}\n",
    "deep_val_data['annotations'] = deep_val_anno\n",
    "deep_val_data['categories'] = all_cate\n",
    "deep_val_data['images'] = deep_val_img\n",
    "with open(FILE_DEEP_VAL_JSON, 'w') as outfile:\n",
    "    json.dump(deep_val_data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모든 Image 파일 한 곳으로 모으기\n",
    "\n",
    "coco dataset의 이미지와 deepfashion 이미지들을 한 폴더로 모두 복사합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed - COCO_IMAGES_TRAIN2017\n",
      "Completed - COCO_IMAGES_VAL2017\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from shutil import copyfile\n",
    "\n",
    "for img_data in coco_train_img:\n",
    "    img_name = img_data['file_name']\n",
    "    img_path = os.path.join(DIR_ROOT_COCO_IMAGES_TRAIN2017, img_name)\n",
    "    new_img_path = os.path.join(DIR_ALL_IMAGES_TRAIN2017, img_name)\n",
    "    if not os.path.exists(new_img_path):\n",
    "        if not os.path.exists(img_path):\n",
    "            print(\"not found\", img_path)\n",
    "        else:\n",
    "            copyfile(img_path, new_img_path)\n",
    "#     if os.path.exists(img_path):\n",
    "#         shutil.move(img_path, new_img_path)\n",
    "        \n",
    "print(\"Completed - COCO_IMAGES_TRAIN2017\")    \n",
    "\n",
    "for img_data in coco_val_img:\n",
    "    img_name = img_data['file_name']\n",
    "    img_path = os.path.join(DIR_ROOT_COCO_IMAGES_VAL2017, img_name)\n",
    "    new_img_path = os.path.join(DIR_ALL_IMAGES_VAL2017, img_name)\n",
    "#     if os.path.exists(img_path):\n",
    "#         shutil.move(img_path, new_img_path)\n",
    "    if not os.path.exists(new_img_path):\n",
    "        if not os.path.exists(img_path):\n",
    "            print(\"not found\", img_path)\n",
    "        else:\n",
    "            copyfile(img_path, new_img_path)\n",
    "            \n",
    "print(\"Completed - COCO_IMAGES_VAL2017\")    \n",
    "        \n",
    "for img_data in deep_train_img:\n",
    "    img_name = img_data['file_name']\n",
    "    img_path = os.path.join(DIR_ROOT_DEEPFASHION_IMAGES, img_name)\n",
    "    new_img_path = os.path.join(DIR_ALL_IMAGES_TRAIN2017, img_name)\n",
    "#     if os.path.exists(img_path):\n",
    "#         shutil.move(img_path, new_img_path)\n",
    "    if not os.path.exists(new_img_path):\n",
    "        if not os.path.exists(img_path):\n",
    "            print(\"not found\", img_path)\n",
    "        else:\n",
    "            copyfile(img_path, new_img_path)\n",
    "            \n",
    "print(\"Completed - DEEPFASHION_IMAGES_TRAIN2017\")    \n",
    "        \n",
    "for img_data in deep_val_img:\n",
    "    img_name = img_data['file_name']\n",
    "    img_path = os.path.join(DIR_ROOT_DEEPFASHION_IMAGES, img_name)\n",
    "    new_img_path = os.path.join(DIR_ALL_IMAGES_VAL2017, img_name)\n",
    "#     if os.path.exists(img_path):\n",
    "#         shutil.move(img_path, new_img_path)\n",
    "    if not os.path.exists(new_img_path):\n",
    "        if not os.path.exists(img_path):\n",
    "            print(\"not found\", img_path)\n",
    "        else:\n",
    "            copyfile(img_path, new_img_path)\n",
    "            \n",
    "print(\"Completed - DEEPFASHION_IMAGES_VAL2017\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
