{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import keras\n",
    "from numpy import array\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Root directory of the project\n",
    "DIR_ROOT = os.getcwd()\n",
    "DIR_DEEPFASHION = \"/Users/luke/Documents/ml_datasets/old/deepfashion/\"\n",
    "DIR_ROOT_DEEPFASHION = os.path.join(DIR_ROOT, DIR_DEEPFASHION)\n",
    "DIR_ROOT_DEEPFASHION_IMAGE = os.path.join(DIR_ROOT, DIR_DEEPFASHION, \"images\")\n",
    "DIR_DEEPFASHION_IMAGE = \"images\"\n",
    "FILE_TRAIN_CSV = os.path.join(DIR_DEEPFASHION, \"main_cate_train2017.csv\")\n",
    "FILE_VAL_CSV = os.path.join(DIR_DEEPFASHION, \"main_cate_val2017.csv\")\n",
    "FILE_CATEGORIES_CSV = os.path.join(DIR_DEEPFASHION, \"main_categories2017.csv\")\n",
    "\n",
    "if not os.path.exists(os.path.join(DIR_DEEPFASHION, DIR_DEEPFASHION_IMAGE)):\n",
    "    os.makedirs(os.path.join(DIR_DEEPFASHION, DIR_DEEPFASHION_IMAGE))\n",
    "    \n",
    "# START_CATEGORY_ID = 100\n",
    "START_CATEGORY_ID = -1 # deepfahsion이 id를 1부터 잡고 있다. 0부터 다시 초기화시키기 위한 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Categories CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path 설정\n",
    "path_list_category_cloth = os.path.join(DIR_DEEPFASHION, \"Anno\", \"list_category_cloth.txt\");\n",
    "\n",
    "# create category object\n",
    "super_categories = [['upper-body', 0], ['lower-body', 1], ['full-body', 2]]\n",
    "super_categories_name_dic = {}\n",
    "super_categories_name_dic[0] = 'upper-body'\n",
    "super_categories_name_dic[1] = 'lower-body'\n",
    "super_categories_name_dic[2] = 'full-body'\n",
    "\n",
    "super_categories_dic = {}\n",
    "with open(path_list_category_cloth) as file_list_category_cloth:\n",
    "    # 첫번째, 두번째 줄은 데이타의 정보를 나타내므로 skip 처리.\n",
    "    next(file_list_category_cloth)\n",
    "    next(file_list_category_cloth)\n",
    "    for index, line in enumerate(file_list_category_cloth):\n",
    "        name = line.strip()[:-1].strip()\n",
    "        super_category_id = int(line.strip()[-1:].strip())+START_CATEGORY_ID\n",
    "        id = index+1+START_CATEGORY_ID\n",
    "        super_categories_dic[id] = super_category_id\n",
    "        \n",
    "dataset_classes = np.asarray(super_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Anno CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start parsing ~\n",
      "skip 2 line (data num, data type)\n",
      "end parsing ~\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "\n",
    "# path 설정\n",
    "path_list_bbox = os.path.join(DIR_DEEPFASHION, \"Anno\", \"list_bbox.txt\");\n",
    "path_list_category_img = os.path.join(DIR_DEEPFASHION, \"Anno\", \"list_category_img.txt\"); \n",
    "\n",
    "# create annotations object\n",
    "dataset_annotations = []\n",
    "print('start parsing ~')\n",
    "\n",
    "# 'img/Sheer_Pleated-Front_Blouse/img_00000001.jpg' '072' '079' '232' '273' 'Blouse'\n",
    "\n",
    "with open(path_list_bbox) as file_path_list_bbox:\n",
    "    with open(path_list_category_img) as file_path_list_category_img:\n",
    "        # 첫번째, 두번째 줄은 데이타의 정보를 나타내므로 skip 처리.\n",
    "        next(file_path_list_bbox)\n",
    "        next(file_path_list_bbox)\n",
    "        next(file_path_list_category_img)\n",
    "        next(file_path_list_category_img)        \n",
    "        print('skip 2 line (data num, data type)')\n",
    "        for index, (line_bbox, line_image) in enumerate(zip(file_path_list_bbox, file_path_list_category_img)):\n",
    "            # each rows\n",
    "            line_image = line_image.split()\n",
    "            line_bbox = line_bbox.split()\n",
    "            image_path = os.path.join(DIR_DEEPFASHION, line_image[0])\n",
    "            \n",
    "            image_format = line_image[0].split('/')[-1].split('.')[1]\n",
    "            image_name = str(index+100000000000+1).zfill(12)+'.'+image_format\n",
    "            new_global_image_path = os.path.join(DIR_ROOT_DEEPFASHION_IMAGE, image_name)\n",
    "            new_local_image_path = os.path.join(DIR_DEEPFASHION_IMAGE, image_name)\n",
    "            \n",
    "            category_id = int(line_image[1])+START_CATEGORY_ID\n",
    "            super_category_id = super_categories_dic[category_id]\n",
    "            super_category_name = super_categories_name_dic[int(super_category_id)]\n",
    "            \n",
    "            # 이미지 파일 유무 확인\n",
    "            #if not os.path.exists(image_path):\n",
    "            #    continue\n",
    "            \n",
    "            # image경로를 통하여 같은 데이타인지 확인\n",
    "            if line_image[0] != line_bbox[0]:\n",
    "                print('error image path')\n",
    "                continue\n",
    "                \n",
    "            # get image size        \n",
    "            image_size = Image.open(image_path).size\n",
    "            if image_size[0]==0 or image_size[1]==0:\n",
    "                print('error image size')\n",
    "                continue\n",
    "            \n",
    "            # coco dataset과 동일한 local 경로로 파일 이동 또는 복사한다\n",
    "            # move file\n",
    "            # shutil.move(image_path, new_global_image_path)\n",
    "            # copy file\n",
    "            if not os.path.exists(new_global_image_path):\n",
    "                copyfile(image_path, new_global_image_path)\n",
    "                \n",
    "            dataset_annotations.append([new_local_image_path, line_bbox[1], line_bbox[2], line_bbox[3], line_bbox[4], super_category_name])\n",
    "            \n",
    "print('end parsing ~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 개수 -  3\n",
      "image 개수 -  289222\n"
     ]
    }
   ],
   "source": [
    "dataset_annotations = np.asarray(dataset_annotations)\n",
    "\n",
    "print('class 개수 - ',len(dataset_classes))\n",
    "print('image 개수 - ',len(dataset_annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Validation Dataset 만들기\n",
    "1.shuffle한다.\n",
    "2.80%는 train set으로 20%는 test set으로 분류한다.\n",
    "    x_train, y_train, x_test, y_test 으로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 dataset에서 80%를 train dataset으로 사용하고 나머지를 test dataset으로 사용합니다.\n",
      "train 개수 -  231377\n",
      "test 개수 -  57845\n",
      "train 샘플 -  full-body\n",
      "test 샘플 -  lower-body\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(dataset_annotations)\n",
    "\n",
    "# 80%는 train set으로 20%는 test set으로 분류\n",
    "threshold = int(dataset_annotations.shape[0] * 0.8)\n",
    "train_annotations = dataset_annotations[:threshold,]\n",
    "test_annotations = dataset_annotations[threshold:,]\n",
    "\n",
    "print(\"전체 dataset에서 80%를 train dataset으로 사용하고 나머지를 test dataset으로 사용합니다.\")\n",
    "print(\"train 개수 - \", train_annotations.shape[0])\n",
    "print(\"test 개수 - \", test_annotations.shape[0])\n",
    "\n",
    "print(\"train 샘플 - \", train_annotations[0][5])\n",
    "print(\"test 샘플 - \", test_annotations[0][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV파일로 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE_TRAIN_CSV\n",
      "FILE_VAL_CSV\n",
      "FILE_CATEGORIES_CSV\n"
     ]
    }
   ],
   "source": [
    "np.savetxt(FILE_TRAIN_CSV, train_annotations, delimiter=\",\", fmt=\"%s\") \n",
    "print('FILE_TRAIN_CSV')\n",
    "np.savetxt(FILE_VAL_CSV, test_annotations, delimiter=\",\", fmt=\"%s\") \n",
    "print('FILE_VAL_CSV')\n",
    "np.savetxt(FILE_CATEGORIES_CSV, dataset_classes, delimiter=\",\", fmt=\"%s\")\n",
    "print('FILE_CATEGORIES_CSV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
