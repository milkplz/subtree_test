{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필요한 라이브러리\n",
    "numpy tensorflow keras skimage\n",
    "```\n",
    "$ pip3 install numpy tensorflow keras skimage\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import keras\n",
    "from numpy import array\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Root directory of the project\n",
    "DIR_ROOT = os.getcwd()\n",
    "DIR_DEEPFASHION = \"../dataset/deepfashion\"\n",
    "DIR_ROOT_DEEPFASHION = os.path.join(DIR_ROOT, DIR_DEEPFASHION)\n",
    "DIR_ROOT_DEEPFASHION_IMAGE = os.path.join(DIR_ROOT, DIR_DEEPFASHION, \"images\")\n",
    "DIR_DEEPFASHION_IMAGE = \"images\"\n",
    "FILE_TRAIN_CSV = os.path.join(DIR_DEEPFASHION, \"instances_train2017.csv\")\n",
    "FILE_VAL_CSV = os.path.join(DIR_DEEPFASHION, \"instances_val2017.csv\")\n",
    "FILE_CATEGORIES_CSV = os.path.join(DIR_DEEPFASHION, \"categories2017.csv\")\n",
    "\n",
    "if not os.path.exists(os.path.join(DIR_DEEPFASHION, DIR_DEEPFASHION_IMAGE)):\n",
    "    os.makedirs(os.path.join(DIR_DEEPFASHION, DIR_DEEPFASHION_IMAGE))\n",
    "    \n",
    "    \n",
    "# START_CATEGORY_ID = 100\n",
    "START_CATEGORY_ID = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepfashion dataset을 load하고,  필요한 데이타를 메모리에 올립니다.\n",
    "Deepfashion dataset에서 category, imagepath, bbox 데이타를 load한다.\n",
    "txt파일들은 모두 첫번째 라인은 list 개수가 적혀 있고, 두번째 라인은 데이타 항목이 적혀 있다.\n",
    "numpy에서 matrix로 load하기 위해 첫번째 라인과 두번째라인 삭제한다. (수동으로)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Categories CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path 설정\n",
    "path_list_category_cloth = os.path.join(DIR_DEEPFASHION, \"Anno\", \"list_category_cloth.txt\");\n",
    "\n",
    "# create category object\n",
    "categories = []\n",
    "categories_dic = {}\n",
    "with open(path_list_category_cloth) as file_list_category_cloth:\n",
    "    # 첫번째, 두번째 줄은 데이타의 정보를 나타내므로 skip 처리.\n",
    "    next(file_list_category_cloth)\n",
    "    next(file_list_category_cloth)\n",
    "    for index, line in enumerate(file_list_category_cloth):\n",
    "        name = line.strip()[:-1].strip()\n",
    "        id = index+1+START_CATEGORY_ID\n",
    "        categories.append([name, id])\n",
    "        categories_dic[id] = name\n",
    "\n",
    "dataset_classes = np.asarray(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Anno CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start parsing ~\n",
      "skip 2 line (data num, data type)\n",
      "end parsing ~\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "\n",
    "# path 설정\n",
    "path_list_bbox = os.path.join(DIR_DEEPFASHION, \"Anno\", \"list_bbox.txt\");\n",
    "path_list_category_img = os.path.join(DIR_DEEPFASHION, \"Anno\", \"list_category_img.txt\"); \n",
    "\n",
    "# create annotations object\n",
    "dataset_annotations = []\n",
    "print('start parsing ~')\n",
    "\n",
    "# 'img/Sheer_Pleated-Front_Blouse/img_00000001.jpg' '072' '079' '232' '273' 'Blouse'\n",
    "\n",
    "with open(path_list_bbox) as file_path_list_bbox:\n",
    "    with open(path_list_category_img) as file_path_list_category_img:\n",
    "        # 첫번째, 두번째 줄은 데이타의 정보를 나타내므로 skip 처리.\n",
    "        next(file_path_list_bbox)\n",
    "        next(file_path_list_bbox)\n",
    "        next(file_path_list_category_img)\n",
    "        next(file_path_list_category_img)        \n",
    "        print('skip 2 line (data num, data type)')\n",
    "        for index, (line_bbox, line_image) in enumerate(zip(file_path_list_bbox, file_path_list_category_img)):\n",
    "            # each rows\n",
    "            line_image = line_image.split()\n",
    "            line_bbox = line_bbox.split()\n",
    "            image_path = os.path.join(DIR_DEEPFASHION, line_image[0])\n",
    "            \n",
    "            image_format = line_image[0].split('/')[-1].split('.')[1]\n",
    "            image_name = str(index+100000000000+1).zfill(12)+'.'+image_format\n",
    "            new_global_image_path = os.path.join(DIR_ROOT_DEEPFASHION_IMAGE, image_name)\n",
    "            new_local_image_path = os.path.join(DIR_DEEPFASHION_IMAGE, image_name)\n",
    "            \n",
    "            category_id = int(line_image[1])+START_CATEGORY_ID\n",
    "            \n",
    "            # 이미지 파일 유무 확인\n",
    "            #if not os.path.exists(image_path):\n",
    "            #    continue\n",
    "            \n",
    "            # image경로를 통하여 같은 데이타인지 확인\n",
    "            if line_image[0] != line_bbox[0]:\n",
    "                print('error image path')\n",
    "                continue\n",
    "                \n",
    "            # get image size        \n",
    "            image_size = Image.open(image_path).size\n",
    "            if image_size[0]==0 or image_size[1]==0:\n",
    "                print('error image size')\n",
    "                continue\n",
    "            \n",
    "            # coco dataset과 동일한 local 경로로 파일 이동 또는 복사한다\n",
    "            # move file\n",
    "            # shutil.move(image_path, new_global_image_path)\n",
    "            # copy file\n",
    "            if not os.path.exists(new_global_image_path):\n",
    "                copyfile(image_path, new_global_image_path)\n",
    "                \n",
    "            dataset_annotations.append([new_local_image_path, line_bbox[1], line_bbox[2], line_bbox[3], line_bbox[4], categories_dic[int(category_id)]])\n",
    "            \n",
    "print('end parsing ~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 개수 -  50\n",
      "image 개수 -  289222\n"
     ]
    }
   ],
   "source": [
    "dataset_annotations = np.asarray(dataset_annotations)\n",
    "\n",
    "print('class 개수 - ',len(dataset_classes))\n",
    "print('image 개수 - ',len(dataset_annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Validation Dataset 만들기\n",
    "1.shuffle한다.\n",
    "2.80%는 train set으로 20%는 test set으로 분류한다.\n",
    "    x_train, y_train, x_test, y_test 으로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 dataset에서 80%를 train dataset으로 사용하고 나머지를 test dataset으로 사용합니다.\n",
      "train 개수 -  231377\n",
      "test 개수 -  57845\n",
      "train 샘플 -  Jacket\n",
      "test 샘플 -  Leggings\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(dataset_annotations)\n",
    "\n",
    "# 80%는 train set으로 20%는 test set으로 분류\n",
    "threshold = int(dataset_annotations.shape[0] * 0.8)\n",
    "train_annotations = dataset_annotations[:threshold,]\n",
    "test_annotations = dataset_annotations[threshold:,]\n",
    "\n",
    "print(\"전체 dataset에서 80%를 train dataset으로 사용하고 나머지를 test dataset으로 사용합니다.\")\n",
    "print(\"train 개수 - \", train_annotations.shape[0])\n",
    "print(\"test 개수 - \", test_annotations.shape[0])\n",
    "\n",
    "print(\"train 샘플 - \", train_annotations[0][5])\n",
    "print(\"test 샘플 - \", test_annotations[0][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV파일로 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE_TRAIN_CSV\n",
      "FILE_VAL_CSV\n",
      "FILE_CATEGORIES_CSV\n"
     ]
    }
   ],
   "source": [
    "np.savetxt(FILE_TRAIN_CSV, train_annotations, delimiter=\",\", fmt=\"%s\") \n",
    "print('FILE_TRAIN_CSV')\n",
    "np.savetxt(FILE_VAL_CSV, test_annotations, delimiter=\",\", fmt=\"%s\") \n",
    "print('FILE_VAL_CSV')\n",
    "np.savetxt(FILE_CATEGORIES_CSV, dataset_classes, delimiter=\",\", fmt=\"%s\")\n",
    "print('FILE_CATEGORIES_CSV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
